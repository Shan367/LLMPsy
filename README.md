# LLMPsy
If you want to ask questions about the native llama large language model, then you need to deploy ollama locally first \n
If you are testing against openai's large language model then you need to set your keys locally now \n
for mac/linux:export OPENAI_API_KEY="your_api_key_here" \n
for windows:setx OPENAI_API_KEY "your_api_key_here" \n
